{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cac7447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# 1. 数据准备\n",
    "def load_poems(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        poems = [line.strip() for line in f.readlines() if len(line.strip()) > 0]\n",
    "    return poems\n",
    "\n",
    "# 2. 数据预处理\n",
    "def build_vocab(poems):\n",
    "    all_chars = [char for poem in poems for char in poem]\n",
    "    counter = Counter(all_chars)\n",
    "    char_freq = sorted(counter.items(), key=lambda x: -x[1])\n",
    "    chars = [char for char, freq in char_freq]\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "    idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
    "    return char_to_idx, idx_to_char, len(chars)\n",
    "\n",
    "def poem_to_indices(poem, char_to_idx):\n",
    "    return [char_to_idx[char] for char in poem]\n",
    "\n",
    "def create_dataset(poems, char_to_idx, seq_length=50):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for poem in poems:\n",
    "        indices = poem_to_indices(poem, char_to_idx)\n",
    "        for i in range(0, len(indices) - seq_length):\n",
    "            inputs.append(indices[i:i+seq_length])\n",
    "            targets.append(indices[i+1:i+seq_length+1])\n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# 3. 模型构建\n",
    "class PoetryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(PoetryModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.num_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n",
    "\n",
    "# 4. 训练函数\n",
    "def train_model(model, dataloader, epochs, learning_rate):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        \n",
    "        for batch, (inputs, targets) in enumerate(dataloader):\n",
    "            hidden = tuple([h.data for h in hidden])\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "            if batch % 10 == 0:\n",
    "                # 保存模型\n",
    "                torch.save(model.state_dict(), 'model/poetry_model_epoch_{}_batch_{}.pth'.format(epoch+1, batch))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# 5. 生成函数\n",
    "def generate_poem(model, idx_to_char, char_to_idx, start_string, \n",
    "                 poem_type='五言', temperature=0.8, max_lines=4):\n",
    "    \"\"\"\n",
    "    生成格式整齐的古诗\n",
    "    poem_type: 五言或七言\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 设置诗句长度\n",
    "    line_length = 5 if poem_type == '五言' else 7\n",
    "    separators = ['，', '。']\n",
    "    \n",
    "    # 初始化输入\n",
    "    input_seq = [char_to_idx[char] for char in start_string]\n",
    "    input_seq = torch.LongTensor(input_seq).unsqueeze(0)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    generated_poem = start_string\n",
    "    current_line_len = len(start_string)\n",
    "    lines = 1\n",
    "    expecting_separator = False\n",
    "    \n",
    "    while lines <= max_lines:\n",
    "        # 生成下一个字符\n",
    "        output, hidden = model(input_seq, hidden)\n",
    "        last_char_logits = output[:, -1, :] / temperature\n",
    "        probabilities = torch.softmax(last_char_logits, dim=-1)\n",
    "        predicted_idx = torch.multinomial(probabilities, 1).item()\n",
    "        predicted_char = idx_to_char[predicted_idx]\n",
    "        while not expecting_separator and predicted_char in separators:\n",
    "            probabilities[0][predicted_idx] = 0.0  # 避免重复\n",
    "            predicted_idx = torch.multinomial(probabilities, 1).item()\n",
    "            predicted_char = idx_to_char[predicted_idx]\n",
    "\n",
    "        \n",
    "        # 行长度控制\n",
    "        if not expecting_separator:\n",
    "            current_line_len += 1\n",
    "            # 达到指定长度时应该生成分隔符\n",
    "            if current_line_len == line_length:\n",
    "                expecting_separator = True\n",
    "        else:\n",
    "            # 检查是否是有效的分隔符\n",
    "            predicted_char='，' if lines%2==1 else '。'\n",
    "            predicted_idx = char_to_idx[predicted_char]\n",
    "            # if predicted_char not in separators:\n",
    "            #     predicted_char = random.choice(separators)\n",
    "            #     predicted_idx = char_to_idx[predicted_char]\n",
    "            lines += 1\n",
    "            current_line_len = 0\n",
    "            expecting_separator = False\n",
    "        generated_poem += predicted_char\n",
    "        input_seq = torch.LongTensor([predicted_idx]).unsqueeze(0)\n",
    "        if lines > max_lines:\n",
    "            break\n",
    "    \n",
    "    # # 后处理，确保最后以句号结束\n",
    "    # if not generated_poem.endswith('。'):\n",
    "    #     generated_poem += '。'\n",
    "    \n",
    "    # 格式化为每行一句\n",
    "    poem_lines = []\n",
    "    current_line = \"\"\n",
    "    for char in generated_poem:\n",
    "        current_line += char\n",
    "        if char in ['，', '。']:\n",
    "            poem_lines.append(current_line)\n",
    "            current_line = \"\"\n",
    "    \n",
    "    # # 确保偶数行（完整的对联）\n",
    "    # if len(poem_lines) % 2 != 0 and len(poem_lines) > 1:\n",
    "    #     poem_lines = poem_lines[:-1]\n",
    "    \n",
    "    return \"\\n\".join(poem_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef9ae17c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "seq_length = 30\n",
    "batch_size = 781*3*2\n",
    "embedding_dim = 64\n",
    "hidden_dim = 64\n",
    "# batch_size = 64\n",
    "# embedding_dim = 256\n",
    "# hidden_dim = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.1\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17a6ef0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 加载数据\n",
    "poems = load_poems('data/poems.txt')  # 替换为你的文件路径\n",
    "char_to_idx, idx_to_char, vocab_size = build_vocab(poems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d21685f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([23430, 30]), torch.Size([23430, 30]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建数据集\n",
    "X, y = create_dataset(poems, char_to_idx, seq_length)\n",
    "X = torch.from_numpy(X).long()\n",
    "y = torch.from_numpy(y).long()\n",
    "dataset = PoemDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc3865e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PoetryModel(vocab_size, embedding_dim, hidden_dim, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21ff31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Batch [0/5], Loss: 8.0736\n",
      "Epoch [1/200], Batch [1/5], Loss: 7.2662\n",
      "Epoch [1/200], Batch [2/5], Loss: 7.6992\n",
      "Epoch [1/200], Batch [3/5], Loss: 7.4551\n",
      "Epoch [1/200], Batch [4/5], Loss: 6.8491\n",
      "Epoch [2/200], Batch [0/5], Loss: 6.7194\n",
      "Epoch [2/200], Batch [1/5], Loss: 6.7208\n",
      "Epoch [2/200], Batch [2/5], Loss: 6.7637\n",
      "Epoch [2/200], Batch [3/5], Loss: 6.6835\n",
      "Epoch [2/200], Batch [4/5], Loss: 6.6630\n",
      "Epoch [3/200], Batch [0/5], Loss: 6.7201\n",
      "Epoch [3/200], Batch [1/5], Loss: 6.6779\n",
      "Epoch [3/200], Batch [2/5], Loss: 6.6843\n",
      "Epoch [3/200], Batch [3/5], Loss: 6.6625\n",
      "Epoch [3/200], Batch [4/5], Loss: 6.6468\n",
      "Epoch [4/200], Batch [0/5], Loss: 6.6387\n",
      "Epoch [4/200], Batch [1/5], Loss: 6.6359\n",
      "Epoch [4/200], Batch [2/5], Loss: 6.6592\n",
      "Epoch [4/200], Batch [3/5], Loss: 6.6361\n",
      "Epoch [4/200], Batch [4/5], Loss: 6.6240\n",
      "Epoch [5/200], Batch [0/5], Loss: 6.6093\n",
      "Epoch [5/200], Batch [1/5], Loss: 6.6104\n",
      "Epoch [5/200], Batch [2/5], Loss: 6.5883\n",
      "Epoch [5/200], Batch [3/5], Loss: 6.5807\n",
      "Epoch [5/200], Batch [4/5], Loss: 6.5594\n",
      "Epoch [6/200], Batch [0/5], Loss: 6.5486\n",
      "Epoch [6/200], Batch [1/5], Loss: 6.5338\n",
      "Epoch [6/200], Batch [2/5], Loss: 6.5142\n",
      "Epoch [6/200], Batch [3/5], Loss: 6.5103\n",
      "Epoch [6/200], Batch [4/5], Loss: 6.4930\n",
      "Epoch [7/200], Batch [0/5], Loss: 6.4797\n",
      "Epoch [7/200], Batch [1/5], Loss: 6.4605\n",
      "Epoch [7/200], Batch [2/5], Loss: 6.4497\n",
      "Epoch [7/200], Batch [3/5], Loss: 6.4319\n",
      "Epoch [7/200], Batch [4/5], Loss: 6.4323\n",
      "Epoch [8/200], Batch [0/5], Loss: 6.4056\n",
      "Epoch [8/200], Batch [1/5], Loss: 6.3909\n",
      "Epoch [8/200], Batch [2/5], Loss: 6.3793\n",
      "Epoch [8/200], Batch [3/5], Loss: 6.3713\n",
      "Epoch [8/200], Batch [4/5], Loss: 6.3494\n",
      "Epoch [9/200], Batch [0/5], Loss: 6.3279\n",
      "Epoch [9/200], Batch [1/5], Loss: 6.3216\n",
      "Epoch [9/200], Batch [2/5], Loss: 6.3029\n",
      "Epoch [9/200], Batch [3/5], Loss: 6.2870\n",
      "Epoch [9/200], Batch [4/5], Loss: 6.2767\n",
      "Epoch [10/200], Batch [0/5], Loss: 6.2553\n",
      "Epoch [10/200], Batch [1/5], Loss: 6.2287\n",
      "Epoch [10/200], Batch [2/5], Loss: 6.2250\n",
      "Epoch [10/200], Batch [3/5], Loss: 6.2047\n",
      "Epoch [10/200], Batch [4/5], Loss: 6.1937\n",
      "Epoch [11/200], Batch [0/5], Loss: 6.1620\n",
      "Epoch [11/200], Batch [1/5], Loss: 6.1556\n",
      "Epoch [11/200], Batch [2/5], Loss: 6.1346\n",
      "Epoch [11/200], Batch [3/5], Loss: 6.1325\n",
      "Epoch [11/200], Batch [4/5], Loss: 6.1061\n",
      "Epoch [12/200], Batch [0/5], Loss: 6.0775\n",
      "Epoch [12/200], Batch [1/5], Loss: 6.0622\n",
      "Epoch [12/200], Batch [2/5], Loss: 6.0565\n",
      "Epoch [12/200], Batch [3/5], Loss: 6.0464\n",
      "Epoch [12/200], Batch [4/5], Loss: 6.0302\n",
      "Epoch [13/200], Batch [0/5], Loss: 5.9910\n",
      "Epoch [13/200], Batch [1/5], Loss: 5.9925\n",
      "Epoch [13/200], Batch [2/5], Loss: 5.9751\n",
      "Epoch [13/200], Batch [3/5], Loss: 5.9518\n",
      "Epoch [13/200], Batch [4/5], Loss: 5.9500\n",
      "Epoch [14/200], Batch [0/5], Loss: 5.9134\n",
      "Epoch [14/200], Batch [1/5], Loss: 5.8941\n",
      "Epoch [14/200], Batch [2/5], Loss: 5.8847\n",
      "Epoch [14/200], Batch [3/5], Loss: 5.8773\n",
      "Epoch [14/200], Batch [4/5], Loss: 5.8632\n",
      "Epoch [15/200], Batch [0/5], Loss: 5.8238\n",
      "Epoch [15/200], Batch [1/5], Loss: 5.8143\n",
      "Epoch [15/200], Batch [2/5], Loss: 5.8045\n",
      "Epoch [15/200], Batch [3/5], Loss: 5.7931\n",
      "Epoch [15/200], Batch [4/5], Loss: 5.7711\n",
      "Epoch [16/200], Batch [0/5], Loss: 5.7410\n",
      "Epoch [16/200], Batch [1/5], Loss: 5.7477\n",
      "Epoch [16/200], Batch [2/5], Loss: 5.7436\n",
      "Epoch [16/200], Batch [3/5], Loss: 5.7194\n",
      "Epoch [16/200], Batch [4/5], Loss: 5.7114\n",
      "Epoch [17/200], Batch [0/5], Loss: 5.6787\n",
      "Epoch [17/200], Batch [1/5], Loss: 5.6843\n",
      "Epoch [17/200], Batch [2/5], Loss: 5.6681\n",
      "Epoch [17/200], Batch [3/5], Loss: 5.6441\n",
      "Epoch [17/200], Batch [4/5], Loss: 5.6466\n",
      "Epoch [18/200], Batch [0/5], Loss: 5.6041\n",
      "Epoch [18/200], Batch [1/5], Loss: 5.5951\n",
      "Epoch [18/200], Batch [2/5], Loss: 5.5867\n",
      "Epoch [18/200], Batch [3/5], Loss: 5.5847\n",
      "Epoch [18/200], Batch [4/5], Loss: 5.5658\n",
      "Epoch [19/200], Batch [0/5], Loss: 5.5310\n",
      "Epoch [19/200], Batch [1/5], Loss: 5.5265\n",
      "Epoch [19/200], Batch [2/5], Loss: 5.5112\n",
      "Epoch [19/200], Batch [3/5], Loss: 5.5000\n",
      "Epoch [19/200], Batch [4/5], Loss: 5.4873\n",
      "Epoch [20/200], Batch [0/5], Loss: 5.4439\n",
      "Epoch [20/200], Batch [1/5], Loss: 5.4572\n",
      "Epoch [20/200], Batch [2/5], Loss: 5.4496\n",
      "Epoch [20/200], Batch [3/5], Loss: 5.4398\n",
      "Epoch [20/200], Batch [4/5], Loss: 5.4253\n",
      "Epoch [21/200], Batch [0/5], Loss: 5.3814\n",
      "Epoch [21/200], Batch [1/5], Loss: 5.3955\n",
      "Epoch [21/200], Batch [2/5], Loss: 5.3808\n",
      "Epoch [21/200], Batch [3/5], Loss: 5.3704\n",
      "Epoch [21/200], Batch [4/5], Loss: 5.3752\n",
      "Epoch [22/200], Batch [0/5], Loss: 5.3267\n",
      "Epoch [22/200], Batch [1/5], Loss: 5.3333\n",
      "Epoch [22/200], Batch [2/5], Loss: 5.3363\n",
      "Epoch [22/200], Batch [3/5], Loss: 5.3234\n",
      "Epoch [22/200], Batch [4/5], Loss: 5.3031\n",
      "Epoch [23/200], Batch [0/5], Loss: 5.2711\n",
      "Epoch [23/200], Batch [1/5], Loss: 5.2802\n",
      "Epoch [23/200], Batch [2/5], Loss: 5.2666\n",
      "Epoch [23/200], Batch [3/5], Loss: 5.2525\n",
      "Epoch [23/200], Batch [4/5], Loss: 5.2578\n",
      "Epoch [24/200], Batch [0/5], Loss: 5.2030\n"
     ]
    }
   ],
   "source": [
    "train_model(model, dataloader, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d136f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PoetryModel(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
    "model.load_state_dict(torch.load('data2/test4model2/poetry_model_epoch_32_batch_0.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0140dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "生成五言诗:\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春虽故乡未，\n",
      "云霞五说与。\n",
      "云倚画树唱，\n",
      "离处隔鸡琴。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春分道只和惠志，\n",
      "聪间王州泪奔高。\n",
      "公朝手驿閒事已，\n",
      "华想家尘凤声来。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春卧稳自近，\n",
      "踟蹰路荣是。\n",
      "名成新词近，\n",
      "东篱影悬仙。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春过山卧兴衰旧，\n",
      "更阑分君臣可愁。\n",
      "离然散卧霜叶叶，\n",
      "青为断处我平归。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春驰栈知春，\n",
      "却在紫是含。\n",
      "祇遥雾势年，\n",
      "缓飘无辞穗。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春接修时遣扫望，\n",
      "枝堂梦得神居处。\n",
      "别后成旅路真踪，\n",
      "不得分分白醉故。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春鸿迎又棹，\n",
      "拾翠谁复响。\n",
      "远来浮世利，\n",
      "唤静为始堪。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春在食迳收前宝，\n",
      "抄得新书已在学。\n",
      "独听病依难照香，\n",
      "天惊后清飞时淅。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春赏古长终，\n",
      "有令自荒干。\n",
      "怡惊蝉展名，\n",
      "江淹散谩恋。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春分取欺炉香儒，\n",
      "清星对善友相未。\n",
      "便牵魂密犹吟影，\n",
      "公退斯得陇残红。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春虽自枯谙，\n",
      "直艇高斋轻。\n",
      "筹难名忽岭，\n",
      "兴后逢时绝。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春锁惊后岸垂钓，\n",
      "风卷喷云梦憾江。\n",
      "素道归路亦醉吟，\n",
      "暖歌尽鹭敲翠枝。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春讲似秋丝，\n",
      "留畔野去故。\n",
      "旌表宜颜古，\n",
      "心同入当欲。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春棹山圆非却屠，\n",
      "接印不独能肃物。\n",
      "竹逐春山尽石江，\n",
      "三泉石影轻曲印。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春虽了露中，\n",
      "红山只庭迟。\n",
      "幽窗前见问，\n",
      "春风春平鼻。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春棹笼霜起竹人，\n",
      "山光生寿老时髦。\n",
      "翅佩烟笼琴生冷，\n",
      "欲色芳受百笑知。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春卧九路赊，\n",
      "今松院恨长。\n",
      "警露吟出格，\n",
      "求化营年知。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春万事搜官事远，\n",
      "两礼岸烟霞高然。\n",
      "云霞曾时心惟来，\n",
      "自由俱汉绿微难。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春谩花光惆，\n",
      "无凭费叟烟。\n",
      "数家芦苇风，\n",
      "茱萸饮数春。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春莫在未为仙照，\n",
      "身无别随鬼流轻。\n",
      "意知更心常玉箸，\n",
      "故帆春幽景未迟。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春虽雁劳影，\n",
      "笛头前畴药。\n",
      "他年翻因佳，\n",
      "珠满明对密。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春鸿一筵金辂知，\n",
      "相喜紫照山上却。\n",
      "处世语锁迷陛戟，\n",
      "逸欢社心何上薄。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春辔长生紫，\n",
      "今日南屦恣。\n",
      "花我清自事，\n",
      "一时过巢无。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春穷隠上薄通理，\n",
      "青衫感台风掌筵。\n",
      "地来天白云迷枕，\n",
      "物气至未诗溪事。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春穷轩为等，\n",
      "夕阳须书人。\n",
      "征体是晨醒，\n",
      "自门前水组。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春未性庾兔憎嫌，\n",
      "乘轩水猿行竹不。\n",
      "窗前日暮花怨是，\n",
      "流才调巾箱学灵。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春卧床头欲，\n",
      "好看日为坐。\n",
      "若贪戏乱早，\n",
      "碧芙江年改。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春谩床戒覆门天，\n",
      "月有梁年红衣客。\n",
      "旧问行意无曾因，\n",
      "轻养通花崦应门。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春成惟贤书，\n",
      "渔浦经疏雨。\n",
      "林间竹有楚，\n",
      "朝浅三山碧。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春棹霁来长近来，\n",
      "日回红云草忆花。\n",
      "有越鸿相渐平十，\n",
      "衡杨柳只垂袂诸。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春分卧棘字，\n",
      "赞色直根琴。\n",
      "越国烟处跻，\n",
      "路上送人疏。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春鳞渥洼骢非载，\n",
      "丛边还闻籍地身。\n",
      "落清终不思知否，\n",
      "莲边春披琼蕴诵。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春驰栈深轮，\n",
      "舐犊自纷纷。\n",
      "花房犹神寻，\n",
      "四鬭棋身性。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春驰调事虽废侍，\n",
      "五有无门掩野何。\n",
      "香急梦寐一无篇，\n",
      "一酒有移泽一鹤。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春赏国息华，\n",
      "海离琴朱更。\n",
      "野色半映空，\n",
      "樵栏翻水乍。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春树蔽郊原上诏，\n",
      "松可涤昏垂赋诗。\n",
      "小离山岳栏隄镜，\n",
      "汎逐我自忻楼驰。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春人成须非，\n",
      "日对逍遥易。\n",
      "西游难池长，\n",
      "依心雁外明。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春驰栈独似千尺，\n",
      "银台上樵里为偶。\n",
      "溪景能是闭人来，\n",
      "莲袅红上重巅浮。\n",
      "\n",
      "以'春'开头的五言诗:\n",
      "春讲因礼映，\n",
      "花情冲湘筠。\n",
      "高节子陵静，\n",
      "疏味谿下荒。\n",
      "\n",
      "生成七言诗:\n",
      "\n",
      "以'春'开头的七言诗:\n",
      "春广怀籁炉烟微，\n",
      "依山掩然拂尘极。\n",
      "背想为别抛与此，\n",
      "迎香泉声酬日夜。\n"
     ]
    }
   ],
   "source": [
    "start_chars = [\"春\"]\n",
    "\n",
    "print(\"\\n生成五言诗:\")\n",
    "for i in range(20):\n",
    "    for start in start_chars:\n",
    "        generated_poem = generate_poem(model, idx_to_char, char_to_idx, start, \n",
    "                                        poem_type='五言', temperature=1)\n",
    "        print(f\"\\n以'{start}'开头的五言诗:\")\n",
    "        print(generated_poem)\n",
    "\n",
    "    print(\"\\n生成七言诗:\")\n",
    "    for start in start_chars:\n",
    "        generated_poem = generate_poem(model, idx_to_char, char_to_idx, start, \n",
    "                                        poem_type='七言', temperature=1)\n",
    "        print(f\"\\n以'{start}'开头的七言诗:\")\n",
    "        print(generated_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4169d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
